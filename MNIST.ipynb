{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUjbHq9URNgCLLAw2P86l8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmelinetsen/deep_learning/blob/master/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdtWp4q6xgN",
        "colab_type": "text"
      },
      "source": [
        "## Do MNIST classifier using numpy and python without CNN and just using plain neural networks. Parts covered in section:\n",
        "\n",
        "\n",
        "*   Scaling the input - part F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WztWPz_aZLm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# loading the mnist data into training and testing data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# getting the first 1000 data and reshaping the dataset for the first 1000\n",
        "# scaling the input to 255\n",
        "img, labels = (x_train[0:1000].reshape(1000,28*28) / 255), y_train[0:1000]\n",
        "\n",
        "# creating an array of zeros \n",
        "one_hot_labels = np.zeros((len(labels), 10))\n",
        "\n",
        "\n",
        "# assigning 1 to where the label would be for that particular array\n",
        "# for example, if the array \n",
        "for i,l in enumerate(labels):\n",
        "  one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "import sys, numpy as np\n",
        "\n",
        "test_img = x_test.reshape(len(x_test), 28*28) / 255\n",
        "test_label = np.zeros((len(y_test), 10))\n",
        "\n",
        "for i,l in enumerate(y_test):\n",
        "    test_label[i][l] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dwl__SE5BE0",
        "colab_type": "text"
      },
      "source": [
        "## Do MNIST classifier using numpy and python without CNN and just using plain neural networks. Parts covered in section:\n",
        "\n",
        "\n",
        "*   Mini batch - part A\n",
        "*   Random weight initializations - part B\n",
        "*   Using RELU activation function - part E\n",
        "*   Using appropriate learning rate - part G\n",
        "*   Testing and training accuracy | plotting results with confusion matrix - part H\n",
        "\n",
        "\n",
        "## Mini batch\n",
        "# Dropout mask, Random weight initalizations (Part A, B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgOr5mgFYqz",
        "colab_type": "code",
        "outputId": "e50465a7-dcdb-46fc-8550-5fdcd96b174f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# setting up relu activation function\n",
        "# would return x if x > 0, else return 0\n",
        "relu = lambda x: (x>0) * x\n",
        "\n",
        "# setting up backprop for relu\n",
        "# returns 1 if input > 0, else return 0\n",
        "relu2deriv = lambda x: x >= 0\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.001\n",
        "\n",
        "# number of iterations\n",
        "iterations = 200\n",
        "\n",
        "# hidden size\n",
        "hidden = 100\n",
        "\n",
        "# pixels per image \n",
        "pixels = 28 * 28\n",
        "\n",
        "# number of labels\n",
        "num_labels = 10\n",
        "\n",
        "# batch size\n",
        "batch_size = 100\n",
        "\n",
        "# weights from layer 0 to 1\n",
        "# initialize random weights\n",
        "weights_0_1 = 0.2 * np.random.random((pixels, hidden)) - 0.1\n",
        "\n",
        "# weights from layer 1 to 2\n",
        "# initialize random weights\n",
        "weights_1_2 = 0.2*np.random.random((hidden,num_labels)) - 0.1\n",
        "\n",
        "# training\n",
        "for j in range(iterations):\n",
        "  error = 0.0\n",
        "  correct_cnt = 0\n",
        "\n",
        "  # iterating through each training image\n",
        "  for i in range(int(len(img) / batch_size)):\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = (i+1) * batch_size\n",
        "    \n",
        "    # layer 0\n",
        "    layer_0 = img[batch_start : batch_end]\n",
        "    # layer 1\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    # adding dropout mask\n",
        "    dropout_mask = np.random.randint(2 , size=layer_1.shape)\n",
        "    layer_1 *= dropout_mask * 2\n",
        "    # layer 2\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "    # MSE between the predicted and actual label value\n",
        "    error += np.sum((labels[batch_start : batch_end] - layer_2) ** 2)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "\n",
        "      # counting how many times the model predicts correctly\n",
        "      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_end+k+1]))\n",
        "\n",
        "      # difference predicted and actual \n",
        "      layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
        "\n",
        "      # backprop & adjusting weights\n",
        "      # adding dropout mask\n",
        "      layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
        "      layer_1_delta *= dropout_mask\n",
        "\n",
        "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "      # print(weights_0_1)\n",
        "\n",
        "  if (j%10 == 0):\n",
        "    test_error = 0.0\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_img)):\n",
        "      layer_0 = test_img[i:i+1]\n",
        "      layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "      layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "\n",
        "      test_error += np.sum((test_label[i:i+1] - layer_2) ** 2)\n",
        "      test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_label[i:i+1]))\n",
        "  \n",
        "    # print(test_error)\n",
        "    sys.stdout.write(\"\\n I:\"+str(j)+ \\\n",
        "                     \" Test-Error:\" + str(test_error/ float(len(test_img)))[0:5] +\\\n",
        "                     \" Test-Accuracy:\" + str(test_correct_cnt/ float(len(test_img)))+\\\n",
        "                     \" Train-Error:\" + str(error/float(len(img)))[0:5] +\\\n",
        "                     \" Train-Accuracy:\" + str(correct_cnt/float(len(img))))\n",
        "    # print(str(test_error/ float(len(test_img)))[0:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " I:0 Test-Error:0.817 Test-Accuracy:0.3742 Train-Error:1.274 Train-Accuracy:0.18\n",
            " I:10 Test-Error:0.550 Test-Accuracy:0.7394 Train-Error:0.587 Train-Accuracy:0.694\n",
            " I:20 Test-Error:0.493 Test-Accuracy:0.7747 Train-Error:0.520 Train-Accuracy:0.752\n",
            " I:30 Test-Error:0.469 Test-Accuracy:0.7867 Train-Error:0.495 Train-Accuracy:0.753\n",
            " I:40 Test-Error:0.454 Test-Accuracy:0.7891 Train-Error:0.472 Train-Accuracy:0.782\n",
            " I:50 Test-Error:0.450 Test-Accuracy:0.7894 Train-Error:0.456 Train-Accuracy:0.777\n",
            " I:60 Test-Error:0.445 Test-Accuracy:0.7874 Train-Error:0.449 Train-Accuracy:0.814\n",
            " I:70 Test-Error:0.445 Test-Accuracy:0.7821 Train-Error:0.439 Train-Accuracy:0.806\n",
            " I:80 Test-Error:0.451 Test-Accuracy:0.7805 Train-Error:0.440 Train-Accuracy:0.8\n",
            " I:90 Test-Error:0.452 Test-Accuracy:0.7796 Train-Error:0.429 Train-Accuracy:0.806\n",
            " I:100 Test-Error:0.448 Test-Accuracy:0.7789 Train-Error:0.439 Train-Accuracy:0.804\n",
            " I:110 Test-Error:0.445 Test-Accuracy:0.7865 Train-Error:0.428 Train-Accuracy:0.819\n",
            " I:120 Test-Error:0.444 Test-Accuracy:0.791 Train-Error:0.439 Train-Accuracy:0.801\n",
            " I:130 Test-Error:0.443 Test-Accuracy:0.793 Train-Error:0.424 Train-Accuracy:0.809\n",
            " I:140 Test-Error:0.438 Test-Accuracy:0.8001 Train-Error:0.414 Train-Accuracy:0.844\n",
            " I:150 Test-Error:0.434 Test-Accuracy:0.8032 Train-Error:0.411 Train-Accuracy:0.831\n",
            " I:160 Test-Error:0.438 Test-Accuracy:0.8009 Train-Error:0.406 Train-Accuracy:0.835\n",
            " I:170 Test-Error:0.432 Test-Accuracy:0.8056 Train-Error:0.407 Train-Accuracy:0.819\n",
            " I:180 Test-Error:0.431 Test-Accuracy:0.8024 Train-Error:0.400 Train-Accuracy:0.839\n",
            " I:190 Test-Error:0.428 Test-Accuracy:0.807 Train-Error:0.408 Train-Accuracy:0.834"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyR_YQz-uBZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d5bbca46-2af7-4a93-c2f2-82523ff76897"
      },
      "source": [
        "# setting up relu activation function\n",
        "# would return x if x > 0, else return 0\n",
        "relu = lambda x: (x>0) * x\n",
        "\n",
        "# setting up backprop for relu\n",
        "# returns 1 if input > 0, else return 0\n",
        "relu2deriv = lambda x: x >= 0\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.001\n",
        "\n",
        "# number of iterations\n",
        "iterations = 200\n",
        "\n",
        "# hidden size\n",
        "hidden = 100\n",
        "\n",
        "# pixels per image \n",
        "pixels = 28 * 28\n",
        "\n",
        "# number of labels\n",
        "num_labels = 10\n",
        "\n",
        "# batch size\n",
        "batch_size = 100\n",
        "\n",
        "# weights from layer 0 to 1\n",
        "# initialize random weights\n",
        "weights_0_1 = 0.2 * np.random.random((pixels, hidden)) - 0.1\n",
        "\n",
        "# weights from layer 1 to 2\n",
        "# initialize random weights\n",
        "weights_1_2 = 0.2*np.random.random((hidden,num_labels)) - 0.1\n",
        "\n",
        "test_err_np = np.array([])\n",
        "test_acc_np = np.array([])\n",
        "train_err_np = np.array([])\n",
        "train_acc_np = np.array([])\n",
        "\n",
        "# training\n",
        "for j in range(iterations):\n",
        "  error = 0.0\n",
        "  correct_cnt = 0\n",
        " \n",
        "  # iterating through each training image\n",
        "  for i in range(int(len(img) / batch_size)):\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = (i+1) * batch_size\n",
        "    \n",
        "    # layer 0\n",
        "    layer_0 = img[batch_start : batch_end]\n",
        "    # layer 1\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    # adding dropout mask\n",
        "    dropout_mask = np.random.randint(2 , size=layer_1.shape)\n",
        "    layer_1 *= dropout_mask * 2\n",
        "    # layer 2\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "    # MSE between the predicted and actual label value\n",
        "    error += np.sum((labels[batch_start : batch_end] - layer_2) ** 2)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "\n",
        "      # counting how many times the model predicts correctly\n",
        "      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_end+k+1]))\n",
        "\n",
        "      # difference predicted and actual \n",
        "      layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
        "\n",
        "      # backprop & adjusting weights\n",
        "      # adding dropout mask\n",
        "      layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
        "      layer_1_delta *= dropout_mask\n",
        "\n",
        "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "      # print(weights_0_1)\n",
        "\n",
        "  \n",
        "  if (j%10 == 0):\n",
        "    test_error = 0.0\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_img)):\n",
        "      layer_0 = test_img[i:i+1]\n",
        "      layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "      layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "\n",
        "      test_error += np.sum((test_label[i:i+1] - layer_2) ** 2)\n",
        "      test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_label[i:i+1]))\n",
        "      \n",
        "    test_err_np = np.append(test_np, [test_error/ float(len(test_img))])\n",
        "    test_acc_np = np.append(test_acc_np, [test_correct_cnt / float(len(test_img))])\n",
        "    train_err_np = np.append(train_err_np, [error/float(len(img))])\n",
        "    train_acc_np = np.append(train_acc_np, [correct_cnt/float(len(img))])\n",
        "    sys.stdout.write(\"\\n I:\"+str(j)+ \\\n",
        "                     \" Test-Error:\" + str(test_error/ float(len(test_img)))[0:5] +\\\n",
        "                     \" Test-Accuracy:\" + str(test_correct_cnt/ float(len(test_img)))+\\\n",
        "                     \" Train-Error:\" + str(error/float(len(img)))[0:5] +\\\n",
        "                     \" Train-Accuracy:\" + str(correct_cnt/float(len(img))))\n",
        "    # print(str(test_error/ float(len(test_img)))[0:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " I:0 Test-Error:0.812 Test-Accuracy:0.3883 Train-Error:1.241 Train-Accuracy:0.176\n",
            " I:10 Test-Error:0.573 Test-Accuracy:0.7056 Train-Error:0.613 Train-Accuracy:0.662\n",
            " I:20 Test-Error:0.515 Test-Accuracy:0.7495 Train-Error:0.542 Train-Accuracy:0.73\n",
            " I:30 Test-Error:0.481 Test-Accuracy:0.7781 Train-Error:0.512 Train-Accuracy:0.746\n",
            " I:40 Test-Error:0.465 Test-Accuracy:0.7951 Train-Error:0.494 Train-Accuracy:0.767\n",
            " I:50 Test-Error:0.452 Test-Accuracy:0.8021 Train-Error:0.472 Train-Accuracy:0.773\n",
            " I:60 Test-Error:0.445 Test-Accuracy:0.8009 Train-Error:0.455 Train-Accuracy:0.784\n",
            " I:70 Test-Error:0.440 Test-Accuracy:0.8015 Train-Error:0.451 Train-Accuracy:0.782\n",
            " I:80 Test-Error:0.448 Test-Accuracy:0.7929 Train-Error:0.452 Train-Accuracy:0.798\n",
            " I:90 Test-Error:0.452 Test-Accuracy:0.7927 Train-Error:0.443 Train-Accuracy:0.792\n",
            " I:100 Test-Error:0.452 Test-Accuracy:0.7965 Train-Error:0.448 Train-Accuracy:0.794\n",
            " I:110 Test-Error:0.448 Test-Accuracy:0.7948 Train-Error:0.441 Train-Accuracy:0.797\n",
            " I:120 Test-Error:0.449 Test-Accuracy:0.7954 Train-Error:0.429 Train-Accuracy:0.803\n",
            " I:130 Test-Error:0.445 Test-Accuracy:0.7893 Train-Error:0.419 Train-Accuracy:0.816\n",
            " I:140 Test-Error:0.440 Test-Accuracy:0.7937 Train-Error:0.426 Train-Accuracy:0.819\n",
            " I:150 Test-Error:0.439 Test-Accuracy:0.7991 Train-Error:0.421 Train-Accuracy:0.824\n",
            " I:160 Test-Error:0.436 Test-Accuracy:0.7988 Train-Error:0.413 Train-Accuracy:0.829\n",
            " I:170 Test-Error:0.434 Test-Accuracy:0.8026 Train-Error:0.397 Train-Accuracy:0.811\n",
            " I:180 Test-Error:0.432 Test-Accuracy:0.8006 Train-Error:0.397 Train-Accuracy:0.836\n",
            " I:190 Test-Error:0.431 Test-Accuracy:0.7977 Train-Error:0.394 Train-Accuracy:0.832"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In4825g8AGlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "43961bc6-a56e-4d11-ad4b-d50f1ea609d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(test_acc_np, color='r', label=\"Testing Accuracy\")\n",
        "ax[0].plot(test_err_np, color='b', label=\"Testing Error\")\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "\n",
        "ax[1].plot(train_acc_np, color='r', label=\"Training Accuracy\")\n",
        "ax[1].plot(train_err_np, color='b', label=\"Training Error\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWbTa8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkxFgkSIFsdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrix \n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(X_val)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(Y_val,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbX93eg9tep",
        "colab_type": "text"
      },
      "source": [
        "c) The code should do basic image augmentations to supplement the training data (not testing data) using keras libraries  (NEW than the deck) - see the image augmentations tried in https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGBeJIh9vYK",
        "colab_type": "text"
      },
      "source": [
        "d) The code should use  3 or more layers for training (not 2 as in example ) - you have to tune and pick number of neurons in your layer and number of layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz_62E4H-APu",
        "colab_type": "text"
      },
      "source": [
        "i) The code should display top common errors like in below link."
      ]
    }
  ]
}