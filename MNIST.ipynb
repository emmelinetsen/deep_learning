{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwUlQ0NzEsrwyaYo66fiY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmelinetsen/deep_learning/blob/master/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdtWp4q6xgN",
        "colab_type": "text"
      },
      "source": [
        "Do MNIST classifier using numpy and python without CNN and just using plain neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OY_EjmM5Xoc",
        "colab_type": "code",
        "outputId": "08d51844-4752-40d5-b5ef-4786a582342e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rz_t3aN5vmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the mnist data into training and testing data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# getting the first 1000 data and reshaping the dataset for the first 1000\n",
        "img, labels = (x_train[0:1000].reshape(1000,28*28) / 255), y_train[0:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WztWPz_aZLm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating an array of zeros \n",
        "one_hot_labels = np.zeros((len(labels), 10))\n",
        "\n",
        "\n",
        "# assigning 1 to where the label would be for that particular array\n",
        "# for example, if the array \n",
        "for i,l in enumerate(labels):\n",
        "  one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "import sys, numpy as np\n",
        "\n",
        "test_img = x_test.reshape(len(x_test), 28*28) / 255\n",
        "test_label = np.zeros((len(y_test), 10))\n",
        "\n",
        "for i,l in enumerate(y_test):\n",
        "    test_label[i][l] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgOr5mgFYqz",
        "colab_type": "code",
        "outputId": "c92f0b0a-35bf-42ec-f05f-8ba49ea5f208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# setting up relu activation function\n",
        "# would return x if x > 0, else return 0\n",
        "relu = lambda x: (x>0) * x\n",
        "\n",
        "# setting up backprop for relu\n",
        "# returns 1 if input > 0, else return 0\n",
        "relu2deriv = lambda x: x >= 0\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.05\n",
        "\n",
        "# number of iterations\n",
        "iterations = 350\n",
        "\n",
        "# hidden size\n",
        "hidden = 40\n",
        "\n",
        "# pixels per image \n",
        "pixels = 28 * 28\n",
        "\n",
        "# number of labels\n",
        "num_labels = 10\n",
        "\n",
        "# weights from layer 0 to 1\n",
        "weights_0_1 = 0.2 * np.random.random((pixels, hidden)) - 0.1\n",
        "\n",
        "# weights from layer 1 to 2\n",
        "weights_1_2 = 0.2*np.random.random((hidden,num_labels)) - 0.1\n",
        "\n",
        "# training\n",
        "for j in range(iterations):\n",
        "  error = 0.0\n",
        "  correct_cnt = 0\n",
        "\n",
        "  # iterating through each training image\n",
        "  for i in range(len(img)):\n",
        "    # layer 0\n",
        "    layer_0 = img[i:i+1]\n",
        "    # layer 1\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    # layer 2\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "    # MSE between the predicted and actual label value\n",
        "    error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
        "\n",
        "    # counting how many times the model predicts correctly\n",
        "    correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
        "\n",
        "    # difference predicted and actual \n",
        "    layer_2_delta = (labels[i:i+1] - layer_2)\n",
        "\n",
        "    # backprop & adjusting weights\n",
        "    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
        "    weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "    weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "\n",
        "    sys.stdout.write(\"\\r I:\"+str(j)+ \\\n",
        "                     \" Train-Err:\" + str(error/float(len(img)))[0:5] +\\\n",
        "                     \" Train-Acc:\" + str(correct_cnt/float(len(img))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " I:108 Train-Err:nan Train-Acc:0.002"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater_equal\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " I:349 Train-Err:nan Train-Acc:0.097"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dwl__SE5BE0",
        "colab_type": "text"
      },
      "source": [
        "a) The code should do mini batch gradient descent along with appropriate learning rate pick appropriate batch size for minibatch (read fastai and other tips and https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6 for example.) see sample https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/ as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdKH_jtP5PdL",
        "colab_type": "text"
      },
      "source": [
        "b) The code should do dropout - try various dropout rates and pick the one which works well. (need not be same for all layers ;-) ). - see https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6 for ideas.\n",
        "\n",
        "The code should initialize the random weights of network properly - see https://docs.google.com/presentation/d/1cYzq7TEGXRAhKF9P0eOI_q01kQAOajl-eS9h3CTGRLg/edit#slide=id.g60fb2717a6_37_196 for more information on how to initialize the random weights before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbX93eg9tep",
        "colab_type": "text"
      },
      "source": [
        "c) The code should do basic image augmentations to supplement the training data (not testing data) using keras libraries  (NEW than the deck) - see the image augmentations tried in https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGBeJIh9vYK",
        "colab_type": "text"
      },
      "source": [
        "d) The code should use  3 or more layers for training (not 2 as in example ) - you have to tune and pick number of neurons in your layer and number of layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1T31Mkc92NX",
        "colab_type": "text"
      },
      "source": [
        "e) The code will continue to use relu activation layer in right places like python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wldvlZZG93oI",
        "colab_type": "text"
      },
      "source": [
        "f) The code should normalize the input as discussed in the class before training (scaling the input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my2WaAYL97N8",
        "colab_type": "text"
      },
      "source": [
        "g) The code should use appropriate learning rate (try out few to find out which one works) - you can use adaptive learning rates like different learning rates per epoch or per mini batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysWpNNCs99Du",
        "colab_type": "text"
      },
      "source": [
        "h) The code should provide appropriate metrics, visualization,  testing and training accuracy etc.,. and plot the results and confusion matrix  (this is important)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz_62E4H-APu",
        "colab_type": "text"
      },
      "source": [
        "i) The code should display top common errors like in below link."
      ]
    }
  ]
}