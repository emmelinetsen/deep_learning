{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCrqyR026dwpLNLrt/crxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmelinetsen/deep_learning/blob/master/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdtWp4q6xgN",
        "colab_type": "text"
      },
      "source": [
        "# Do MNIST classifier using numpy and python without CNN and just using plain neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WztWPz_aZLm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "f7d5aaab-5dd4-43fa-a05e-6eb5b297d980"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# loading the mnist data into training and testing data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# getting the first 1000 data and reshaping the dataset for the first 1000\n",
        "img, labels = (x_train[0:1000].reshape(1000,28*28) / 255), y_train[0:1000]\n",
        "\n",
        "# creating an array of zeros \n",
        "one_hot_labels = np.zeros((len(labels), 10))\n",
        "\n",
        "\n",
        "# assigning 1 to where the label would be for that particular array\n",
        "# for example, if the array \n",
        "for i,l in enumerate(labels):\n",
        "  one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "import sys, numpy as np\n",
        "\n",
        "test_img = x_test.reshape(len(x_test), 28*28) / 255\n",
        "test_label = np.zeros((len(y_test), 10))\n",
        "\n",
        "for i,l in enumerate(y_test):\n",
        "    test_label[i][l] = 1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dwl__SE5BE0",
        "colab_type": "text"
      },
      "source": [
        "## Mini batch, Dropout mask, Random weight initalizations (Part A, B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JgOr5mgFYqz",
        "colab_type": "code",
        "outputId": "e50465a7-dcdb-46fc-8550-5fdcd96b174f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# setting up relu activation function\n",
        "# would return x if x > 0, else return 0\n",
        "relu = lambda x: (x>0) * x\n",
        "\n",
        "# setting up backprop for relu\n",
        "# returns 1 if input > 0, else return 0\n",
        "relu2deriv = lambda x: x >= 0\n",
        "\n",
        "# learning rate\n",
        "alpha = 0.001\n",
        "\n",
        "# number of iterations\n",
        "iterations = 200\n",
        "\n",
        "# hidden size\n",
        "hidden = 100\n",
        "\n",
        "# pixels per image \n",
        "pixels = 28 * 28\n",
        "\n",
        "# number of labels\n",
        "num_labels = 10\n",
        "\n",
        "# batch size\n",
        "batch_size = 100\n",
        "\n",
        "# weights from layer 0 to 1\n",
        "# initialize random weights\n",
        "weights_0_1 = 0.2 * np.random.random((pixels, hidden)) - 0.1\n",
        "\n",
        "# weights from layer 1 to 2\n",
        "# initialize random weights\n",
        "weights_1_2 = 0.2*np.random.random((hidden,num_labels)) - 0.1\n",
        "\n",
        "# training\n",
        "for j in range(iterations):\n",
        "  error = 0.0\n",
        "  correct_cnt = 0\n",
        "\n",
        "  # iterating through each training image\n",
        "  for i in range(int(len(img) / batch_size)):\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = (i+1) * batch_size\n",
        "    \n",
        "    # layer 0\n",
        "    layer_0 = img[batch_start : batch_end]\n",
        "    # layer 1\n",
        "    layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "    # adding dropout mask\n",
        "    dropout_mask = np.random.randint(2 , size=layer_1.shape)\n",
        "    layer_1 *= dropout_mask * 2\n",
        "    # layer 2\n",
        "    layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "    # MSE between the predicted and actual label value\n",
        "    error += np.sum((labels[batch_start : batch_end] - layer_2) ** 2)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "\n",
        "      # counting how many times the model predicts correctly\n",
        "      correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_end+k+1]))\n",
        "\n",
        "      # difference predicted and actual \n",
        "      layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
        "\n",
        "      # backprop & adjusting weights\n",
        "      # adding dropout mask\n",
        "      layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
        "      layer_1_delta *= dropout_mask\n",
        "\n",
        "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "      # print(weights_0_1)\n",
        "\n",
        "  if (j%10 == 0):\n",
        "    test_error = 0.0\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_img)):\n",
        "      layer_0 = test_img[i:i+1]\n",
        "      layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "      layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "\n",
        "      test_error += np.sum((test_label[i:i+1] - layer_2) ** 2)\n",
        "      test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_label[i:i+1]))\n",
        "  \n",
        "    # print(test_error)\n",
        "    sys.stdout.write(\"\\n I:\"+str(j)+ \\\n",
        "                     \" Test-Error:\" + str(test_error/ float(len(test_img)))[0:5] +\\\n",
        "                     \" Test-Accuracy:\" + str(test_correct_cnt/ float(len(test_img)))+\\\n",
        "                     \" Train-Error:\" + str(error/float(len(img)))[0:5] +\\\n",
        "                     \" Train-Accuracy:\" + str(correct_cnt/float(len(img))))\n",
        "    # print(str(test_error/ float(len(test_img)))[0:5])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " I:0 Test-Error:0.817 Test-Accuracy:0.3742 Train-Error:1.274 Train-Accuracy:0.18\n",
            " I:10 Test-Error:0.550 Test-Accuracy:0.7394 Train-Error:0.587 Train-Accuracy:0.694\n",
            " I:20 Test-Error:0.493 Test-Accuracy:0.7747 Train-Error:0.520 Train-Accuracy:0.752\n",
            " I:30 Test-Error:0.469 Test-Accuracy:0.7867 Train-Error:0.495 Train-Accuracy:0.753\n",
            " I:40 Test-Error:0.454 Test-Accuracy:0.7891 Train-Error:0.472 Train-Accuracy:0.782\n",
            " I:50 Test-Error:0.450 Test-Accuracy:0.7894 Train-Error:0.456 Train-Accuracy:0.777\n",
            " I:60 Test-Error:0.445 Test-Accuracy:0.7874 Train-Error:0.449 Train-Accuracy:0.814\n",
            " I:70 Test-Error:0.445 Test-Accuracy:0.7821 Train-Error:0.439 Train-Accuracy:0.806\n",
            " I:80 Test-Error:0.451 Test-Accuracy:0.7805 Train-Error:0.440 Train-Accuracy:0.8\n",
            " I:90 Test-Error:0.452 Test-Accuracy:0.7796 Train-Error:0.429 Train-Accuracy:0.806\n",
            " I:100 Test-Error:0.448 Test-Accuracy:0.7789 Train-Error:0.439 Train-Accuracy:0.804\n",
            " I:110 Test-Error:0.445 Test-Accuracy:0.7865 Train-Error:0.428 Train-Accuracy:0.819\n",
            " I:120 Test-Error:0.444 Test-Accuracy:0.791 Train-Error:0.439 Train-Accuracy:0.801\n",
            " I:130 Test-Error:0.443 Test-Accuracy:0.793 Train-Error:0.424 Train-Accuracy:0.809\n",
            " I:140 Test-Error:0.438 Test-Accuracy:0.8001 Train-Error:0.414 Train-Accuracy:0.844\n",
            " I:150 Test-Error:0.434 Test-Accuracy:0.8032 Train-Error:0.411 Train-Accuracy:0.831\n",
            " I:160 Test-Error:0.438 Test-Accuracy:0.8009 Train-Error:0.406 Train-Accuracy:0.835\n",
            " I:170 Test-Error:0.432 Test-Accuracy:0.8056 Train-Error:0.407 Train-Accuracy:0.819\n",
            " I:180 Test-Error:0.431 Test-Accuracy:0.8024 Train-Error:0.400 Train-Accuracy:0.839\n",
            " I:190 Test-Error:0.428 Test-Accuracy:0.807 Train-Error:0.408 Train-Accuracy:0.834"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbX93eg9tep",
        "colab_type": "text"
      },
      "source": [
        "c) The code should do basic image augmentations to supplement the training data (not testing data) using keras libraries  (NEW than the deck) - see the image augmentations tried in https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGBeJIh9vYK",
        "colab_type": "text"
      },
      "source": [
        "d) The code should use  3 or more layers for training (not 2 as in example ) - you have to tune and pick number of neurons in your layer and number of layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1T31Mkc92NX",
        "colab_type": "text"
      },
      "source": [
        "e) The code will continue to use relu activation layer in right places like python code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wldvlZZG93oI",
        "colab_type": "text"
      },
      "source": [
        "f) The code should normalize the input as discussed in the class before training (scaling the input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my2WaAYL97N8",
        "colab_type": "text"
      },
      "source": [
        "g) The code should use appropriate learning rate (try out few to find out which one works) - you can use adaptive learning rates like different learning rates per epoch or per mini batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysWpNNCs99Du",
        "colab_type": "text"
      },
      "source": [
        "h) The code should provide appropriate metrics, visualization,  testing and training accuracy etc.,. and plot the results and confusion matrix  (this is important)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz_62E4H-APu",
        "colab_type": "text"
      },
      "source": [
        "i) The code should display top common errors like in below link."
      ]
    }
  ]
}